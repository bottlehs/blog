---
templateKey: blog-post
title: AI 모델 양자화 스마트폰에서도 돌아가는 AI의 비밀
date: 2025-11-05T00:00:00.000Z
category: ai
description:
  AI 모델 양자화의 개념과 작동 원리를 설명합니다. QAT, PTQ, GPTQ, AWQ 비교와 실무 적용, 성능 분석까지 알아봅니다.
tags:
  - 양자화
  - Quantization
  - 모델 압축
  - 모델 최적화
  - AI
  - LLM
  - 모바일 AI
  - 엣지 AI
---

![AI 모델 양자화 스마트폰에서도 돌아가는 AI의 비밀](/assets/ai.png "AI 모델 양자화 스마트폰에서도 돌아가는 AI의 비밀")

GPT-4는 강력하지만 수백 GB의 메모리와 고성능 GPU가 필요하다. 일반 스마트폰이나 작은 서버에서는 실행하기 어렵다. 양자화(Quantization)는 이런 문제를 해결하는 기술이다. 모델의 정밀도를 낮춰 크기를 줄이고 속도를 높인다. 이 글은 양자화가 무엇인지, 어떻게 작동하는지, 그리고 실무에서 어떻게 활용할 수 있는지 설명한다.

## 양자화란 무엇인가?

양자화는 모델의 가중치(파라미터)를 낮은 정밀도로 변환해 모델 크기를 줄이고 실행 속도를 높이는 기법이다. 32비트 부동소수점을 8비트나 4비트 정수로 변환한다.

### 정밀도별 비교

**32비트 부동소수점 (FP32)**
- 범위: 매우 넓음
- 정밀도: 높음
- 크기: 1개 파라미터당 4바이트
- 예시: 0.12345678

**16비트 부동소수점 (FP16)**
- 범위: 넓음
- 정밀도: 중간
- 크기: 1개 파라미터당 2바이트
- 예시: 0.12346 (반올림)

**8비트 정수 (INT8)**
- 범위: 제한적 (-128 ~ 127)
- 정밀도: 낮음
- 크기: 1개 파라미터당 1바이트
- 예시: 12 (스케일링 후)

**4비트 정수 (INT4)**
- 범위: 매우 제한적 (-8 ~ 7)
- 정밀도: 매우 낮음
- 크기: 1개 파라미터당 0.5바이트
- 예시: 1 (스케일링 후)

### 양자화의 효과

**모델 크기 감소**
- FP32 → INT8: 4배 감소
- FP32 → INT4: 8배 감소

**예시: GPT-3.5 (1750억 파라미터)**
- FP32: 약 700GB
- INT8: 약 175GB
- INT4: 약 87.5GB

**실행 속도 향상**
- 메모리 사용량 감소로 속도 향상
- 저정밀도 연산이 하드웨어에서 더 빠름
- 배치 처리 시 더 큰 배치 크기 가능

**비용 절감**
- 클라우드 서버 비용 감소
- 모바일 디바이스에서 실행 가능
- 엣지 디바이스 배포 가능

## 양자화의 작동 원리

양자화는 연속적인 값을 이산적인 값으로 변환하는 과정이다.

### 기본 양자화 공식

```
양자화된 값 = round(원본 값 / scale) + zero_point
```

**scale (스케일)**: 원본 범위를 양자화 범위로 매핑하는 비율
**zero_point (영점)**: 원본의 0이 양자화 공간에서 어디에 해당하는지

### 예시: FP32 → INT8 변환

**원본 값들**
```
[0.1, 0.5, 1.2, 2.8, -0.3, -1.5]
```

**1단계: 범위 확인**
- 최댓값: 2.8
- 최솟값: -1.5
- 범위: -1.5 ~ 2.8

**2단계: 스케일 계산**
```
scale = (max - min) / (255 - 0)
scale = (2.8 - (-1.5)) / 255
scale = 4.3 / 255 ≈ 0.0169
```

**3단계: 영점 계산**
```
zero_point = 0 - (min / scale)
zero_point = 0 - (-1.5 / 0.0169)
zero_point ≈ 89
```

**4단계: 양자화**
```
0.1 → round(0.1 / 0.0169) + 89 ≈ 95
0.5 → round(0.5 / 0.0169) + 89 ≈ 119
1.2 → round(1.2 / 0.0169) + 89 ≈ 160
2.8 → round(2.8 / 0.0169) + 89 ≈ 255
-0.3 → round(-0.3 / 0.0169) + 89 ≈ 71
-1.5 → round(-1.5 / 0.0169) + 89 ≈ 0
```

**결과**
```
[95, 119, 160, 255, 71, 0]
```

### 역양자화 (Dequantization)

실제 추론 시에는 양자화된 값을 다시 원래 범위로 변환한다.

```
원본 값 ≈ (양자화된 값 - zero_point) × scale
```

**예시**
```
95 → (95 - 89) × 0.0169 ≈ 0.101
160 → (160 - 89) × 0.0169 ≈ 1.20
```

## 양자화 방법 비교

### 1. Post-Training Quantization (PTQ)

학습이 완료된 모델을 양자화하는 방법이다.

**특징**
- 추가 학습 불필요
- 빠른 적용 가능
- 정확도 손실 다소 큼

**과정**
1. 학습 완료된 모델 준비
2. 캘리브레이션 데이터로 분포 분석
3. 스케일과 영점 계산
4. 양자화 적용

**장점**
- 즉시 적용 가능
- 추가 학습 시간 없음
- 간단한 구현

**단점**
- 정확도 손실이 클 수 있음
- 복잡한 모델에서는 성능 저하 큼

### 2. Quantization-Aware Training (QAT)

학습 과정에서 양자화를 고려해 학습하는 방법이다.

**특징**
- 학습 중 양자화 시뮬레이션
- 높은 정확도 유지
- 추가 학습 시간 필요

**과정**
1. 학습 중 양자화 연산 시뮬레이션
2. 양자화 오차를 역전파로 학습
3. 양자화에 강건한 가중치 학습
4. 최종 양자화 적용

**장점**
- 높은 정확도 유지
- 복잡한 모델에도 효과적
- 최적의 양자화 결과

**단점**
- 추가 학습 시간 필요
- 학습 비용 증가

### 3. GPTQ

GPT 모델에 특화된 양자화 방법이다.

**특징**
- 레이어별 순차 양자화
- 높은 정확도 유지
- GPT 모델에 최적화

**과정**
1. 레이어를 하나씩 순차 처리
2. 각 레이어의 양자화 오차 최소화
3. 이전 레이어의 오차를 다음 레이어에서 보정

**장점**
- GPT 모델에 최적화
- 높은 정확도
- 4비트 양자화도 가능

**단점**
- 처리 시간이 오래 걸림
- GPT 외 모델에는 부적합

### 4. AWQ (Activation-aware Weight Quantization)

활성화 값을 고려한 양자화 방법이다.

**특징**
- 중요한 가중치 보호
- 활성화 분포 분석
- 높은 정확도

**과정**
1. 활성화 값 분포 분석
2. 중요한 가중치 식별
3. 중요 가중치는 고정밀도 유지
4. 나머지만 양자화

**장점**
- 높은 정확도 유지
- 효율적인 메모리 사용
- 다양한 모델에 적용 가능

**단점**
- 분석 시간 필요
- 구현 복잡도 높음

### 양자화 방법 비교표

| 구분 | PTQ | QAT | GPTQ | AWQ |
|------|-----|-----|------|-----|
| **학습 필요** | ❌ | ✅ | ❌ | ❌ |
| **속도** | 빠름 | 느림 | 중간 | 중간 |
| **정확도** | ⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ |
| **적용 난이도** | 쉬움 | 어려움 | 중간 | 중간 |
| **모델 호환성** | 모든 모델 | 모든 모델 | GPT 전용 | 모든 모델 |
| **4비트 지원** | 제한적 | 가능 | 가능 | 가능 |

## 실무 활용 사례

### 1. 모바일 앱에 LLM 통합

**목표**: 스마트폰에서 오프라인으로 LLM 실행

**방법**
- GPT-3.5를 INT8로 양자화
- 모델 크기: 700GB → 175GB
- 추가 최적화로 50GB까지 축소 가능

**결과**
- iPhone 15 Pro에서 실행 가능
- 응답 시간: 2-5초
- 오프라인 작동 가능

**도구**
- Core ML (iOS)
- TensorFlow Lite (Android)
- ONNX Runtime

### 2. 엣지 디바이스 배포

**목표**: IoT 디바이스나 임베디드 시스템에 AI 모델 배포

**방법**
- 작은 LLM (7B 파라미터)을 INT4로 양자화
- 모델 크기: 28GB → 3.5GB
- Raspberry Pi 4에서 실행 가능

**결과**
- 로컬에서 즉시 응답
- 네트워크 연결 불필요
- 개인정보 보호 강화

### 3. 클라우드 서버 비용 절감

**목표**: 서버 비용을 줄이면서 성능 유지

**방법**
- GPT-3.5를 INT8로 양자화
- 더 작은 GPU로 실행 가능
- 배치 크기 증가

**결과**
- GPU 비용 50-70% 절감
- 응답 속도 2-3배 향상
- 동시 처리량 증가

**비용 비교**
- FP32: A100 GPU 8개, 월 $8,000
- INT8: A100 GPU 2개, 월 $2,000

### 4. 실시간 음성 인식

**목표**: 실시간으로 음성을 텍스트로 변환

**방법**
- Whisper 모델을 INT8로 양자화
- 모델 크기: 1.5GB → 400MB
- CPU만으로도 실시간 처리 가능

**결과**
- 지연 시간 50% 감소
- 배터리 사용량 감소
- 저사양 기기에서도 작동

### 5. 브라우저에서 AI 실행

**목표**: 웹 브라우저에서 직접 AI 모델 실행

**방법**
- 작은 LLM을 INT8로 양자화
- WebAssembly로 변환
- 브라우저에서 실행

**결과**
- 서버 없이 클라이언트에서 실행
- 개인정보 보호 강화
- 서버 비용 제로

**도구**
- TensorFlow.js
- ONNX.js
- WebGPU 활용

## 양자화 성능 분석

### 정확도 손실

양자화는 정확도를 일정 부분 손실시킬 수 있다.

**일반적인 정확도 손실**
- FP32 → FP16: 0-1% 손실
- FP32 → INT8: 1-3% 손실
- FP32 → INT4: 3-10% 손실

**작업별 영향**
- 간단한 작업 (분류): 적은 손실
- 복잡한 작업 (생성): 큰 손실 가능

**예시: GPT-3.5 양자화**
- FP32: 정확도 100%
- INT8: 정확도 97-98%
- INT4: 정확도 90-95%

### 속도 향상

양자화는 실행 속도를 향상시킨다.

**일반적인 속도 향상**
- FP32 → FP16: 1.5-2배
- FP32 → INT8: 2-4배
- FP32 → INT4: 4-8배

**하드웨어별 차이**
- GPU: INT8 가속 지원 시 큰 향상
- CPU: INT8 가속 제한적
- 모바일: INT8 가속 지원 기기 증가

### 메모리 사용량

양자화는 메모리 사용량을 크게 줄인다.

**메모리 감소**
- FP32 → FP16: 50% 감소
- FP32 → INT8: 75% 감소
- FP32 → INT4: 87.5% 감소

**예시: GPT-3.5 (1750억 파라미터)**
- FP32: 700GB RAM
- INT8: 175GB RAM
- INT4: 87.5GB RAM

## 실무 활용 가이드

### 양자화 방법 선택 기준

1. **정확도 요구사항**
   - 높은 정확도 필요: QAT 또는 AWQ
   - 적당한 정확도: GPTQ 또는 PTQ

2. **시간 제약**
   - 빠른 적용 필요: PTQ
   - 시간 여유 있음: QAT

3. **모델 종류**
   - GPT 모델: GPTQ
   - 다른 모델: PTQ 또는 AWQ

4. **리소스 제약**
   - 학습 불가: PTQ, GPTQ, AWQ
   - 학습 가능: QAT

### 양자화 도구

**Hugging Face**
- `transformers` 라이브러리
- `optimum` 라이브러리 (양자화 최적화)

**예시 코드**
```python
from transformers import AutoModelForCausalLM
from optimum.onnxruntime import ORTModelForCausalLM
from optimum.onnxruntime.configuration import QuantizationConfig

# 모델 로드
model = AutoModelForCausalLM.from_pretrained("gpt2")

# 양자화 설정
quantization_config = QuantizationConfig(
    is_static=False,
    format="onnx",
    mode="dynamic"
)

# 양자화 적용
quantized_model = ORTModelForCausalLM.from_pretrained(
    "gpt2",
    export=True,
    quantization_config=quantization_config
)
```

**GGML**
- C++ 기반 양자화 도구
- 모바일 및 엣지 디바이스에 최적화

**TensorRT**
- NVIDIA GPU 전용
- 높은 성능 최적화

### 단계별 양자화 가이드

#### 1단계: 모델 준비
- 학습 완료된 모델 확보
- 모델 크기와 구조 확인

#### 2단계: 양자화 방법 선택
- 요구사항에 맞는 방법 선택
- 정확도 vs 속도 vs 비용 고려

#### 3단계: 캘리브레이션 데이터 준비
- PTQ의 경우 대표적인 데이터 샘플 준비
- 100-1000개 샘플 권장

#### 4단계: 양자화 실행
- 선택한 도구로 양자화 수행
- 진행 상황 모니터링

#### 5단계: 평가
- 테스트 데이터로 정확도 측정
- 속도와 메모리 사용량 확인

#### 6단계: 최적화
- 필요시 재양자화
- 하이퍼파라미터 조정

## 주의사항과 한계

### 1. 정확도 손실

양자화는 항상 정확도를 일부 손실시킨다.

**대응 방안**
- 중요한 레이어는 고정밀도 유지
- 혼합 정밀도 사용 (일부 INT8, 일부 FP16)
- QAT로 학습해 정확도 보완

### 2. 하드웨어 호환성

모든 하드웨어가 저정밀도 연산을 지원하는 것은 아니다.

**지원 현황**
- 최신 GPU: INT8 가속 지원
- CPU: 제한적 지원
- 모바일: 최신 칩셋만 지원

**대응 방안**
- 타겟 하드웨어 확인
- 폴백 옵션 준비 (FP16 사용)

### 3. 양자화 노이즈

양자화 과정에서 발생하는 노이즈가 누적될 수 있다.

**대응 방안**
- 레이어별 양자화 (GPTQ)
- 중요한 레이어 보호 (AWQ)
- 정확한 스케일 계산

### 4. 모델별 차이

모든 모델이 동일하게 양자화되는 것은 아니다.

**차이 요인**
- 모델 구조
- 가중치 분포
- 작업 복잡도

**대응 방안**
- 모델별 최적 방법 선택
- 실험을 통한 검증

## 양자화의 미래

### 1. 하드웨어 가속

양자화된 모델을 위한 전용 하드웨어가 등장하고 있다.

**전망**
- INT4 전용 AI 칩
- 양자화 최적화 프로세서
- 저전력 AI 디바이스

### 2. 자동 양자화

AI가 스스로 최적의 양자화 방법을 찾는 기술이 개발되고 있다.

**전망**
- 자동 양자화 파이프라인
- 정확도 손실 최소화 알고리즘
- 실시간 양자화 최적화

### 3. 더 낮은 정밀도

2비트, 1비트 양자화 연구가 진행 중이다.

**전망**
- 극단적 모델 압축
- 모바일에서도 대형 모델 실행
- 엣지 AI의 폭발적 성장

## FAQ

**Q: 양자화는 정확도를 얼마나 손실하나요?**  
A: 일반적으로 FP32 → INT8은 1-3%, INT4는 3-10% 정도 손실됩니다. 하지만 QAT나 GPTQ 같은 고급 방법을 사용하면 손실을 최소화할 수 있습니다. 작업의 복잡도와 모델 구조에 따라 다르므로, 실제 테스트를 통해 확인하는 것이 중요합니다.

**Q: 어떤 양자화 방법을 선택해야 하나요?**  
A: 빠른 적용이 필요하고 정확도가 크게 중요하지 않으면 PTQ를, 높은 정확도가 필요하면 QAT나 GPTQ를 선택하세요. GPT 모델은 GPTQ가, 다른 모델은 AWQ가 적합합니다. 시간과 비용을 고려해 선택하는 것이 좋습니다.

**Q: 양자화한 모델을 스마트폰에서 실행할 수 있나요?**  
A: 네, 가능합니다. INT8로 양자화하면 대부분의 최신 스마트폰에서 실행할 수 있습니다. INT4까지 사용하면 더 작은 모델도 가능합니다. Core ML (iOS)나 TensorFlow Lite (Android)를 사용하면 쉽게 배포할 수 있습니다.

**Q: 양자화는 비용을 얼마나 절감하나요?**  
A: 모델 크기가 4배 줄어들면 메모리 비용도 비슷하게 줄어듭니다. GPU 비용은 50-70% 절감 가능하며, 더 작은 GPU로도 실행할 수 있어 초기 투자 비용도 줄어듭니다. 클라우드 서버 비용은 월 수천 달러에서 수백 달러로 감소할 수 있습니다.

**Q: 양자화는 모든 모델에 적용 가능한가요?**  
A: 대부분의 모델에 적용 가능하지만, 모델 구조와 가중치 분포에 따라 효과가 다릅니다. 트랜스포머 기반 모델은 일반적으로 양자화에 잘 맞지만, 일부 모델은 정확도 손실이 클 수 있습니다. 실제 테스트를 통해 확인하는 것이 중요합니다.

**Q: 양자화와 모델 압축의 차이는 무엇인가요?**  
A: 양자화는 정밀도를 낮춰 크기를 줄이는 방법이고, 모델 압축은 가지치기(Pruning), 지식 증류(Knowledge Distillation) 등 다양한 기법을 포함합니다. 양자화는 모델 압축 기법 중 하나이며, 다른 방법과 함께 사용하면 더 효과적입니다.

